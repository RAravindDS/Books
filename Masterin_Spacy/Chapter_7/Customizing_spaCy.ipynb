{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03bc35f5-45b1-4fb9-9dd9-6193c3a12c9c",
   "metadata": {},
   "source": [
    "<center><h1>ùì¨ùìæùìºùìΩùì∏ùì∂ùì≤ùîÉùì≤ùì∑ùì∞ ùìºùìπùì™ùìíùîÇ ùì∂ùì∏ùì≠ùìÆùìµ</h1></center>\n",
    "    \n",
    "[**coDes**](https://github.com/PacktPublishing/Mastering-spaCy/tree/main/Chapter07)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba2ef43-eaa3-4e44-8348-6843bc4f9bdb",
   "metadata": {},
   "source": [
    "In this chapter we are going to learn train, store, and use custom statistical pipeline components. you will also learn how to make the best use of Prodigy, the annotation tool this chapter, we are going to disucss this topics: \n",
    "\n",
    "* [**Getting started with data preparation**](#Data-Preparation)\n",
    "* [**Annotating and preparing data**](#Annotating-and-preparing-data)\n",
    "* Updating an existing pipeline component\n",
    "* [**Training a pipeline component from scratch**](#Training-a-pipeline-component-from-scratch)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01196442-2dee-4a4c-8243-8ad4c2399dc4",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "In the previous chapter, we looked at statistical model like ner, pos tagger, and more. But in this chapter we customize this model based on particular domain, why we need to customize? However, sometimes, we work on very specific domains that spaCy models didn't see during training. \n",
    "\n",
    "For Example: twitter data has lot's of hastags, it's not a sentence its a phrase and it also has a non regular words also, this things are not seen by spaCy training. spaCy is trained with grammatical english sentences. Another example is the medical domain. The medical domain contains many entities, such as drug, disease, and chemical compound names. These entities are not expected to be recognized by spaCy's NER model because it has no disease or drug entity labels. NER does not know anything about the medical domain at all.\n",
    "\n",
    "Training your custom models requires time and effort. Before even starting the training process, you should decide whether the training is really necessary. To determine whether you really need custom training, you will need to ask yourself the following questions:\n",
    "\n",
    "* Do spaCy models perform well enough on your data?\n",
    "* Does your domain include many labels that are absent in spaCy models?\n",
    "* Is there a pre-trained model/application in GitHub or elsewhere already?\n",
    "\n",
    "\n",
    "**Do spaCy models perform well enough on your data?**\n",
    "\n",
    "If your models performs well enough (above 0.75), then you can customize the model by another spaCy component. For\n",
    "example, let's say we work on the navigation domain and we have utterances\n",
    "such as the following:\n",
    "\n",
    "```\n",
    "navigate to my home\n",
    "navigate to Oxford Street\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e7dd380-6da9-4ad4-8126-6c44398eb70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n",
      "(Oxford Street,)\n"
     ]
    }
   ],
   "source": [
    "import spacy \n",
    "import en_core_web_md \n",
    "from spacy.matcher import Matcher \n",
    "\n",
    "nlp = en_core_web_md.load() \n",
    "matcher = Matcher(nlp.vocab) \n",
    "\n",
    "doc = nlp(\"navigate to my home\")\n",
    "print(doc.ents)\n",
    "\n",
    "doc2 = nlp(\"navigate to Oxford Street\")\n",
    "print(doc2.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "951737d3-67ce-46b2-b50b-9cfc210cc43e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ORG'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2.ents[0].label_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2dc20c8d-f6ea-428d-8459-15b4ffb034d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Companies, agencies, institutions, etc.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('ORG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a1187f-8648-4333-93ab-e9d359a23271",
   "metadata": {},
   "source": [
    "Here we need `home`, `oxford street` as `GPE`, but we get organization. We want this entity to be recognized as GPE, a location. Here, we can train\n",
    "NER further to recognize street names as GPE, as well as also recognizing\n",
    "some location words, such as work, home, and my mama's house, as GPE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5f829b-6a07-4549-aade-3412fda2e79b",
   "metadata": {},
   "source": [
    "**Does your domain include many labels that are absent in spaCy models?** \n",
    "\n",
    "For instance, in the preceding newspaper example, only one entity label,\n",
    "vehicle, is missing from the spaCy's NER model's labels. Other entity types\n",
    "are recognized. In this case, you don't need custom training.\n",
    "\n",
    "Consider the medical domain again. The entities are diseases, symptoms,\n",
    "drugs, dosages, chemical compound names, and so on. This is a specialized\n",
    "and long list of entities. Obviously, for the medical domain, you require\n",
    "custom model training.\n",
    "\n",
    "If we need custom model training, we usually follow these steps:\n",
    "1. Collect your data.\n",
    "2. Annotate your data.\n",
    "3. Decide to update an existing model or train a model from scratch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25ab06f-a73e-4bba-a767-3b6ea324ea77",
   "metadata": {},
   "source": [
    "## Annotating and preparing data\n",
    "\n",
    "\n",
    "Before training the model, we need to collect the data from various resources. After collection of data, we need to annotate the data. spaCy training works with`Json` file. \n",
    "\n",
    "Example of annotated data: \n",
    "```Python \n",
    "annotations  = {\n",
    "                \"sentence\": \"I visited JFK Airport.\"\n",
    "                \"entities\": {\n",
    "                    \"label\": \"LOC\"\n",
    "                    \"value\": \"JFK Airport\"\n",
    "                    }\n",
    "                }\n",
    "```\n",
    "\n",
    "Writing down JSON files manually can be error-prone and time-consuming.\n",
    "Hence, in this section, we'll also see spaCy's annotation tool, Prodigy, along\n",
    "with an open source data annotation tool, Brat. Prodigy is not open source or\n",
    "free, but we will go over how it works to give you a better view of how\n",
    "annotation tools work in general. Brat is open source and immediately\n",
    "available for your use.\n",
    "\n",
    "### Annotating Data with Brat \n",
    "\n",
    "Another annotation tool is Brat, which is a free and web-based tool for [**text\n",
    "annotation**](https://brat.nlplab.org/introduction.html). . After the annotation session is finished, Brat dumps\n",
    "a JSON of annotated data as well.\n",
    "\n",
    "### spaCy Training Data Format \n",
    "\n",
    "As we remarked earlier, spaCy training code works with JSON file format.\n",
    "Let's see the details of training the data format.\n",
    "\n",
    "For the NER, you need to provide a list of pairs of sentences and their\n",
    "annotations. Each annotation should include the entity type, the start position\n",
    "of the entity in terms of characters, and the end position of the entity in terms\n",
    "of characters. Let's see an example of a dataset:\n",
    "\n",
    "```Python \n",
    "training_data = [\n",
    "    (\"I will visit you in Munich.\", {\"entities\": [(20, 26, \"GPE\")]}),\n",
    "    (\"I'm going to Victoria's house.\", {\"entities\": [(13, 23, \"PERSON\"),(24, 29, \"GPE\")]}), \n",
    "    (\"I go there.\", {\"entities\": []})\n",
    "] \n",
    "\n",
    "```\n",
    "\n",
    "We cannot feed the raw text and annotations directly to spaCy. Instead, we\n",
    "need to create an Example object for each training example. Let's see the\n",
    "code:\n",
    "\n",
    "```Python \n",
    "from spacy.training import Example\n",
    "\n",
    "doc = nlp(\"I will visit you in Munich.\") \n",
    "\n",
    "annotations = {'entities': [ (20, 26, 'GPE') ]} \n",
    "example_sent = Example.from_dict(doc, annotations)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1321cb2d-cc6d-4b4b-a856-6160d33c9d0f",
   "metadata": {},
   "source": [
    "**Training the spaCy model (procedure)** \n",
    "1. First, we'll disable all the other statistical pipeline components, including the POS tagger and the dependency parser.\n",
    "2. We'll feed our domain examples to the training procedure.\n",
    "3. We'll evaluate the new NER model.\n",
    "\n",
    "Also, we will learn how to do the following:\n",
    "\n",
    "1. Save the updated NER model to disk.\n",
    "2. Read the updated NER model when we want to use it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6087f7b-5981-47d1-b466-cc1444d70c88",
   "metadata": {},
   "source": [
    "#### Let's Train our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f845c262-f10b-432e-8099-c834ec54fafe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First step is to disable the other statistical pipeline components \n",
    "\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']  # Pipes: ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer']\n",
    "nlp.disable_pipes(*other_pipes)\n",
    "\n",
    "# Another way of writing this code is as follows:\n",
    "\n",
    "# other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "# with nlp.disable_pipes(*other_pipes):\n",
    "      # training code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4ba792-136b-4bf5-8c3f-015f51e8e456",
   "metadata": {},
   "source": [
    "We know spaCy is a neural network model, to trian the NN, we need to give some sorts of weights. Then we will do multiple epochs because\n",
    "showing an example only once is not enough. At each iteration, we shuffle the\n",
    "training data so that the order of the training data does not matter. This\n",
    "shuffling of training data helps train the neural network thoroughly. We will use **SGD** SGD starts from a random point on the loss function and travels\n",
    "down its slope in steps until it reaches the lowest point of that function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baab85d-d252-45b3-9af5-ad09b74c3ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import spacy \n",
    "import random  # for random shuffling \n",
    "import en_core_web_md \n",
    "from spacy.matcher import Matcher \n",
    "from spacy.training import Example\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "matcher = Matcher(nlp.vocab) \n",
    "\n",
    "# let's add the training data (structure should be like this) \n",
    "trainset = [\n",
    "    (\"navigate home\", {\"entities\": [(9,13, \"GPE\")]}),\n",
    "    (\"navigate to office\", {\"entities\": [(12,18, \"GPE\")]}),\n",
    "    (\"navigate\", {\"entities\": []}),\n",
    "    (\"navigate to Oxford Street\", {\"entities\": [(12, 25, \"GPE\")]})\n",
    "]\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "# disable other pipes \n",
    "other_pipes = [ pipe for pipe in nlp.pipe_names if pipe != 'ner'] \n",
    "\n",
    "with nlp.disable_pipes(*other_pipes): \n",
    "    optimizer = nlp.create_optimizer()  # it creates a optimizer object \n",
    "    \n",
    "    for i in range(epochs): \n",
    "        random.shuffle(trainset)  # we need to shuffle every set for each iteration to avoid overfitting \n",
    "        losses = {}  # it stores the losses \n",
    "        \n",
    "        for text, annotations in trainset:\n",
    "            doc = nlp(text)\n",
    "            example = Example.from_dict(doc, annotations)   # we need to give like examples \n",
    "        \n",
    "            # actual trainig here only\n",
    "            nlp.update([example], sgd = optimizer, losses = losses, drop = 0.2)\n",
    "        print('Iteration'+ str(i) + str(':') + str(' Losses:') + str(' : ') + str(losses.get('ner')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07335acc-131a-4abd-b597-3e5c6ac2c6df",
   "metadata": {},
   "source": [
    "It's actually works, In my computer it not workinggg :( "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e019fc8-0406-40bc-b074-16f55d7648e4",
   "metadata": {},
   "source": [
    "```Python\n",
    "nlp = spacy.blank(\"en\")  # spacy.load('en', disable = ['ner'])\n",
    "ner = nlp.create_pipe(\"ner\") \n",
    "ner.from_disk('navi_ner') \n",
    "\n",
    "nlp.add_pipe(ner, 'navi_ner') \n",
    "\n",
    "print(nlp.meta['pipeline'])  # you can load the model like this \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844809c0-aeff-4675-9cfe-e338e27a15e3",
   "metadata": {},
   "source": [
    "## Training a pipeline component from scratch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a45c898-4227-4c3f-a1ac-a0db82a2dc78",
   "metadata": {},
   "source": [
    "In the previous section, we saw how to update the existing NER component\n",
    "according to our data. In this section, we will create a brand-new NER\n",
    "component for the medicine domain.\n",
    "\n",
    "Let's start with a small dataset to understand the training procedure. Then\n",
    "we'll be experimenting with a real medical NLP dataset. The following\n",
    "sentences belong to the medicine domain and include medical entities such as\n",
    "drug and disease names:\n",
    "\n",
    "The following code block shows how to train an NER component from\n",
    "scratch. As we mentioned before, it's better to create our own NER rather\n",
    "than updating spaCy's default NER model as medical entities are not\n",
    "recognized by spaCy's NER component at all. Let's see the code and also\n",
    "compare it to the code from the previous section. We'll go step by step:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cde8928-f53f-4304-9f7f-4504daa98709",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import spacy \n",
    "import random  # for random shuffling \n",
    "import en_core_web_md \n",
    "from spacy.matcher import Matcher \n",
    "from spacy.training import Example\n",
    "\n",
    "\n",
    "train_set = [\n",
    "    (\"Methylphenidate is effectively used in treating children with epilepsy and ADHD.\",\n",
    "     {\"entities\": [(0, 15, \"DRUG\"), (62, 70, \"DISEASE\"), (75, 79, \"DISEASE\")]}),\n",
    "    (\"Patients were followed up for 6 months.\", {\"entities\": []}),\n",
    "    (\"Antichlamydial antibiotics may be useful for during coronary-artery disease.\", {\"entities\": [(0, 26, \"DRUG\"), (52, 75, \"DIS\")]})\n",
    "]\n",
    "\n",
    "\n",
    "# create a blank model \n",
    "nlp = spacy.blank('en')\n",
    "\n",
    "# create a pipe \n",
    "ner = nlp.add_pipe('ner')\n",
    "\n",
    "# set of entities (unique) \n",
    "entities = [\"DIS\", \"DRUG\"]\n",
    "\n",
    "# adding unique label to the nlp pipe \n",
    "for ent in entities:  \n",
    "    ner.add_label(ent)\n",
    "    \n",
    "epochs = 100\n",
    "\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner'] \n",
    "l = []\n",
    "with nlp.disable_pipes(*other_pipes): \n",
    "    optimizer = nlp.begin_training()  # it initializes the ner model with weigts 0, hence it forget everything it learned before (we need blank model)\n",
    "    \n",
    "    \n",
    "    for i in range(epochs):\n",
    "        random.shuffle(train_set)\n",
    "        losses = {}\n",
    "        \n",
    "        for text, annotation in train_set: \n",
    "            doc = nlp.make_doc(text)  # making a text to container \n",
    "            \n",
    "            example = Example.from_dict(doc, annotation)  # we need to give a batch of objects so,\n",
    "            \n",
    "            # training starts \n",
    "            nlp.update([example], sgd = optimizer, losses = losses, drop = 0.5)\n",
    "          \n",
    "        print('Iteration'+ str(i) + str(':') + str(' Losses:') + str(' : ') + str(losses.get('ner'))) \n",
    "        l.append(losses)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e18752-9031-4e7d-8747-bbeea3fa5293",
   "metadata": {},
   "source": [
    "The output will be \n",
    "\n",
    "<img src='images/sup.png' width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0700230-8d1e-4eb8-a107-f63b66b94995",
   "metadata": {},
   "source": [
    "## Working with a real-world dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d9d4fd-41fb-44a1-89e4-1d3e1733c267",
   "metadata": {},
   "source": [
    "In this section, we will train on a real-world corpus. We will train an NER\n",
    "model on the CORD-19 corpus provided by the Allen Institute for AI\n",
    "(https://allenai.org/). This is an open challenge for text miners to extract\n",
    "information from this dataset to help medical professionals around the world\n",
    "fight against Corona disease. CORD-19 is an open source dataset that is\n",
    "collected from over 500,000 scholarly articles about Corona disease. The\n",
    "training set consists of 20 annotated medical text samples:\n",
    "\n",
    "```\n",
    "The antiviral drugs amantadine and rimantadine inhibit a viral\n",
    "ion channel (M2 protein), thus inhibiting replication of the\n",
    "influenza A virus.[86] These drugs are sometimes effective\n",
    "against influenza A if given early in the infection but are\n",
    "ineffective against influenza B viruses, which lack the M2 drug\n",
    "target.[160] Measured resistance to amantadine and rimantadine\n",
    "in American isolates of H3N2 has increased to 91% in 2005.[161]\n",
    "This high level of resistance may be due to the easy\n",
    "availability of amantadines as part of over-the-counter cold\n",
    "remedies in countries such as China and Russia,[162] and their\n",
    "use to prevent outbreaks of influenza in farmed poultry.[163]\n",
    "[164] The CDC recommended against using M2 inhibitors during the\n",
    "2005‚Äì06 influenza season due to high levels of drug resistance.\n",
    "[165]\n",
    "```\n",
    "\n",
    "As we see from this example, real-world medical text can be quite long,\n",
    "and it can include many medical terms and entities. Nouns, verbs, and\n",
    "entities are all related to the medicine domain. Entities can be numbers\n",
    "(91%), number and units (100 ng/ml, 25 microg/ml), number-letter\n",
    "combinations (H3N2), abbreviations (CDC), and also compound words\n",
    "(qRT-PCR, PE-labeled).\n",
    "\n",
    "\n",
    "The medical entities come in several shapes (numbers, number and letter\n",
    "combinations, and compounds) as well as being very domain-specific.\n",
    "Hence, a medical text is very different from everyday spoken/written\n",
    "language and definitely needs custom training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c65a71-0857-4506-9b53-f6fe7291b0b8",
   "metadata": {},
   "source": [
    "**The training data will availabe in this folder**, take this\n",
    "\n",
    "[**all the codes are available here**](https://colab.research.google.com/drive/1yz7i0GcADho46JNXnwd0422dXRanPz5W#scrollTo=T43slgxddiYg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8075da7a-b50c-4f4e-8334-5c2e1d7c6ac7",
   "metadata": {},
   "source": [
    "```Python\n",
    "import json \n",
    "import spacy \n",
    "import random  # for random shuffling \n",
    "import en_core_web_md \n",
    "from spacy.matcher import Matcher \n",
    "from spacy.training import Example\n",
    "\n",
    "\n",
    "train_set = [\n",
    "    (\"Methylphenidate is effectively used in treating children with epilepsy and ADHD.\",\n",
    "     {\"entities\": [(0, 15, \"DRUG\"), (62, 70, \"DISEASE\"), (75, 79, \"DISEASE\")]}),\n",
    "    (\"Patients were followed up for 6 months.\", {\"entities\": []}),\n",
    "    (\"Antichlamydial antibiotics may be useful for during coronary-artery disease.\", {\"entities\": [(0, 26, \"DRUG\"), (52, 75, \"DIS\")]})\n",
    "]\n",
    "\n",
    "\n",
    "# create a blank model \n",
    "nlp = spacy.blank('en')\n",
    "\n",
    "# create a pipe \n",
    "ner = nlp.add_pipe('ner')\n",
    "\n",
    "# set of entities (unique) \n",
    "entities = [\"DIS\", \"DRUG\"]\n",
    "\n",
    "# adding unique label to the nlp pipe \n",
    "for ent in entities:  \n",
    "    ner.add_label(ent)\n",
    "    \n",
    "epochs = 100\n",
    "\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner'] \n",
    "l = []\n",
    "with nlp.disable_pipes(*other_pipes): \n",
    "    optimizer = nlp.begin_training()  # it initializes the ner model with weigts 0, hence it forget everything it learned before (we need blank model)\n",
    "    \n",
    "    \n",
    "    for i in range(epochs):\n",
    "        random.shuffle(train_set)\n",
    "        losses = {}\n",
    "        \n",
    "        for text, annotation in train_set: \n",
    "            doc = nlp.make_doc(text)  # making a text to container \n",
    "            \n",
    "            example = Example.from_dict(doc, annotation)  # we need to give a batch of objects so,\n",
    "            \n",
    "            # training starts \n",
    "            nlp.update([example], sgd = optimizer, losses = losses, drop = 0.5)\n",
    "          \n",
    "        print('Iteration'+ str(i) + str(':') + str(' Losses:') + str(' : ') + str(losses.get('ner'))) \n",
    "        l.append(losses)\n",
    "        \n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
