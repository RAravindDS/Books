{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86daef23-3fac-4f77-ac0e-960c5162d582",
   "metadata": {},
   "source": [
    "<center><h1>ùì°ùìæùìµùìÆ ùìëùì™ùìºùìÆùì≠ ùì∂ùì™ùìΩùì¨ùì±ùì≤ùì∑ùì∞üíñ</h1\n",
    "    \\></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900d5ab3-0788-41e2-9151-5499d1a04b7e",
   "metadata": {},
   "source": [
    "Rule based information extraction is indispensable(‡Æ§‡Æµ‡Æø‡Æ∞‡Øç‡Æï‡Øç‡Æï ‡ÆÆ‡ØÅ‡Æü‡Æø‡ÆØ‡Ææ‡Æ§) for any spacy pipeline, without using any statistical model we will extract the information from this. We will discuss how to extract words using morphological features, POS tags, regex and other spacy features as well. We will use Matcher object in spacy to extract the text using this. \n",
    "\n",
    "There are many types that helps to do this task, let's discuss this one by one! \n",
    "\n",
    "* Token-based matching\n",
    "* PhraseMatcher\n",
    "* EntityRuler\n",
    "* Combining spaCy models and matchers\n",
    "\n",
    "\n",
    "## Token-based matching\n",
    "\n",
    "It just but using the regex we can extract the text easily. But using the regex is little complicated now, because if you want to extract any specific thing you need to mention lot's of things in the regex. But in spaCy you don't need to worry on this things, because the spaCy Matcher object gives the clean code and readable code, let's look at "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1245a366-2d0e-416d-949d-cd8364795f33",
   "metadata": {},
   "source": [
    "```Python \n",
    "import spacy \n",
    "from spacy.matcher import Matcher  # importing matcher class\n",
    "\n",
    "nlp = spacy.load('en_core_web_md') \n",
    "doc = nlp(\"Good morning, I want to reserve a ticket.\")\n",
    "\n",
    "matcher = Matcher(nlp.vocab)  # initialized the vocabulary for the matcher object (this is the usual way to do this) \n",
    "pattern = [[{\"LOWER\": \"good\"}, {\"LOWER\": \"morning\"}, {\"IS_PUNCT\": True}]]\n",
    "\n",
    "matcher.add(\"morningGreeting\",pattern)  # \"morningGreeting\" -> is name of this rule\n",
    "\n",
    "matches = matcher(doc) \n",
    "\n",
    "for match_id, start, end in matches:  # the maches object gives triplets, so we are extracting that\n",
    "    m_span = doc[start:end] \n",
    "    print(start, end, m_span.text)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea2ec369-a58e-4633-ad61-5f3eb2b5e199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3 Good morning,\n"
     ]
    }
   ],
   "source": [
    "import spacy \n",
    "from spacy.matcher import Matcher  \n",
    "\n",
    "nlp = spacy.load('en_core_web_md') \n",
    "doc = nlp(\"Good morning, I want to reserve a ticket.\")\n",
    "\n",
    "matcher = Matcher(nlp.vocab) \n",
    "pattern = [[{\"LOWER\": \"good\"}, {\"LOWER\": \"morning\"}, {\"IS_PUNCT\": True}]]\n",
    "\n",
    "matcher.add(\"morningGreeting\",pattern) \n",
    "\n",
    "matches = matcher(doc) \n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    m_span = doc[start:end] \n",
    "    print(start, end, m_span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec89112f-3a0b-4de0-974b-702e4011b083",
   "metadata": {},
   "source": [
    "**To get better results for this notebook, just restart and run for each cell, coz we are tagging patterns to same matcher so it will lead lot's of outputs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83535d27-160d-445f-a524-77c6dec3979f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3 Good morning,\n",
      "14 17 good evening!\n"
     ]
    }
   ],
   "source": [
    "# let's add more patterns here on the same line\n",
    "\n",
    "doc = nlp(\"Good morning, I want to reserve a ticket. I will then say good evening!\")\n",
    "\n",
    "matcher = Matcher(nlp.vocab) \n",
    "pattern1 = [ [{'LOWER': 'good'}, {'LOWER': 'morning'}, {'IS_PUNCT': True}] ] \n",
    "pattern2 = [ [{'LOWER': 'good'}, {'LOWER': 'evening'}, {'IS_PUNCT': True}] ] \n",
    "\n",
    "matcher.add('morningGreeting', pattern1) \n",
    "matcher.add('eveningGreeting', pattern2) \n",
    "\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    # pattern_name = doc.vocab_strings[match_id]  # helps to get the pattern name \n",
    "    m_span = doc[start:end] \n",
    "    print( start, end, m_span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f208c7de-104c-4e7b-9d20-6fdb53d13bc5",
   "metadata": {},
   "source": [
    "<img src=\"images/all.png\" width=\"600\"/> \n",
    "\n",
    "WE just seen only **LOWER**, let's see some of this things"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329aa93a-4bb1-4de7-8981-11097e1fc8e8",
   "metadata": {},
   "source": [
    "The **Orth** and **TEXT** are same, it just extracts the similar patterns, let's see\n",
    "\n",
    "```Python \n",
    "pattern = [ {'TEXT': 'Bill'} ]\n",
    "pattern - [ {'ORTH': 'Bill'} ] # both are same don't worry \n",
    "```\n",
    "\n",
    "Let's see **LENGTH** artibute ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7e352f1-8bbd-4c29-866a-0100df7a6f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 17 I\n",
      "14 17 a\n",
      "14 17 .\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I bought a pineapple.\") \n",
    "\n",
    "matcher = Matcher(nlp.vocab) \n",
    "pattern = [ [{'LENGTH': 1}] ]  # Length is used for specifiying the token length \n",
    "\n",
    "matcher.add('oneshort', pattern) \n",
    "matches = matcher(doc) \n",
    "\n",
    "for a, b, c in matches: \n",
    "    docs = doc[b:c] \n",
    "    print(start, end, docs.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dbdc6f-d06a-4c5c-b090-363e522ec6ca",
   "metadata": {},
   "source": [
    "Let's see about **IS_ALPHA**, **IS_ASCIII** and **IS_DIGIT**. This features are usefull to find the numbers in text. Let's see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9115780c-9209-4f45-a0bf-8944d57765e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 4 2\n",
      "3 5 2 apples\n",
      "5 6 .\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(\"I met him at 2 o'clock.\")\n",
    "doc2 = nlp(\"He brought me 2 apples.\")\n",
    "\n",
    "pattern = [{\"IS_DIGIT\": True},{\"IS_ALPHA\": True}]  # is digit and is alpha -> True \n",
    "\n",
    "matcher.add(\"numberAndPlainWord\", [pattern])\n",
    "matcher(doc1)\n",
    "\n",
    "matches = matcher(doc2)\n",
    "\n",
    "for mid, start, end in matches: \n",
    "    print(start, end, doc2[start:end])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4219e873-fae4-492f-9a54-79b805241523",
   "metadata": {},
   "source": [
    "IS_LOWER, IS_UPPER, and IS_TITLE are useful attributes for\n",
    "recognizing the token's casing. IS_UPPER is True if the token is all\n",
    "uppercase letters and IS_TITLE is True if the token starts with a capital\n",
    "letter. IS_LOWER is True if the token is all lowercase letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "813311f3-601b-4205-b593-a8af783e0414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 6 SPAM\n",
      "7 8 .\n",
      "15 16 .\n",
      "22 23 SUE\n",
      "23 24 !\n",
      "24 25 !\n",
      "25 26 !\n",
      "26 27 !\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Take me out of your SPAM list. We never asked you to contact me. If you write again we'll SUE!!!!\")\n",
    "\n",
    "pattern = [{\"IS_UPPER\": True}]\n",
    "matcher.add(\"capitals\", [pattern])\n",
    "matches = matcher(doc)\n",
    "\n",
    "for mid, start, end in matches:\n",
    "    print(start, end, doc[start:end])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ca119a-2000-4ff9-96f5-941a6a2c758c",
   "metadata": {},
   "source": [
    "IS_PUNCT, IS_SPACE, and IS_STOP are usually used in patterns that\n",
    "include some helper tokens and correspond to punctuation, space, and\n",
    "stopword tokens (stopwords are common words of a language that do not\n",
    "carry much information, such as a, an, and the in English).\n",
    "IS_SENT_START is another useful attribute; it matches sentence start\n",
    "tokens. Here's a pattern for sentences that start with can and the second word\n",
    "has a capitalized first letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3378157-85a2-461a-9222-f58fa18a927a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2 Can Sally\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(\"Can you swim?\")\n",
    "doc2 = nlp(\"Can Sally swim?\")\n",
    "\n",
    "pattern = [{\"IS_SENT_START\": True, \"LOWER\": \"can\"},{\"IS_TITLE\": True}]\n",
    "matcher.add(\"canThenCapitalized\", [pattern])\n",
    "matcher(doc)\n",
    "\n",
    "matches = matcher(doc2)\n",
    "\n",
    "mid, start, end = matches[0]\n",
    "print(start, end, doc2[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f15148aa-ba69-4c3e-b7e9-4157f2d216de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 Will\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Will you go there?\")\n",
    "pattern = [{\"IS_SENT_START\": True, \"TAG\": \"MD\"}]  # look at chapter 3 for this MD model auxilary verbs. \n",
    "matcher.add('summa', [pattern])\n",
    "matches = matcher(doc)\n",
    "\n",
    "mid, start, end = matches[0]\n",
    "print(start, end, doc[start:end])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "86ac8559-a80c-486c-827d-078007c07cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aravindan R\n",
      "R\n",
      "R\n"
     ]
    }
   ],
   "source": [
    "NAME_PATTERN = [[{'POS': 'PROPN'}, {'POS': 'PROPN'}, {'POS': 'PROPN'}]]\n",
    "\n",
    "doc = nlp(\"My name is Aravindan R \") \n",
    "\n",
    "matcher.add('names', NAME_PATTERN) \n",
    "matches = matcher(doc) \n",
    "\n",
    "for mid, start, end in matches: \n",
    "    d = doc[start:end]\n",
    "    print(d.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdca7282-2304-4e83-bb86-85d9e55b1569",
   "metadata": {},
   "source": [
    "**See our custom extraction, this is how we need to do the extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "798ee07c-86af-45be-93fb-455dd69d931f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aravindan ! loves\n",
      "0\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "import spacy \n",
    "from spacy.matcher import Matcher \n",
    "matcher = Matcher(nlp.vocab) \n",
    "\n",
    "pattern = [{'LOWER': 'aravindan'}, {'IS_PUNCT': True}, {'LOWER': 'loves'}]\n",
    "\n",
    "matcher.add('summa', [pattern]) \n",
    "\n",
    "doc = nlp('aravindan ! loves') \n",
    "\n",
    "matches = matcher(doc)\n",
    "for ids, start, end in matches: \n",
    "    print(doc[start:end])\n",
    "    print(start)\n",
    "    print('ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59a81ea-403e-4854-b6e9-9fad1b6bc452",
   "metadata": {},
   "source": [
    "### Regex Support for token based matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4a0696-e2a5-4614-a96c-6883b19b0538",
   "metadata": {},
   "source": [
    "In the first example we just just two differnt patterns in two different lines, instead of writing two different lines, we can use **or**, **in** like python let's see... \n",
    "\n",
    "<img src=\"images/comp.png\" width=\"600\"/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f0c0aec-99f2-42d6-812f-58b0f3ee0b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3 Good morning,\n",
      "10 13 good evening!\n"
     ]
    }
   ],
   "source": [
    "import spacy \n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "matcher = Matcher(nlp.vocab)\n",
    "doc = nlp(\"Good morning, I'm here. I'll say good evening!!\") \n",
    "\n",
    "pattern = [[ \n",
    "    {'LOWER': 'good'}, \n",
    "    {'LOWER': {'IN': ['morning', 'evening']}}, \n",
    "    {'IS_PUNCT': True} \n",
    "]]\n",
    "\n",
    "matcher.add('in', pattern) \n",
    "matches = matcher(doc) \n",
    "\n",
    "for i, start, end in matches: \n",
    "    print(start,end, doc[start:end])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5331850d-4402-4cfb-abc8-9216837236f9",
   "metadata": {},
   "source": [
    "The **Comparison Operator** are usually go with **lenght**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1caa84a3-2880-4a5a-9e51-1d22bf158247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 4 Trichotillomania\n",
      "12 13 prescribed\n",
      "14 15 Psychosomatic\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I suffered from Trichotillomania when I was in college. The doctor prescribed me Psychosomatic medicine.\")\n",
    "pattern = [{\"LENGTH\": {\">=\" : 10}}]\n",
    "\n",
    "matcher.add(\"longWords\", [pattern])\n",
    "matches = matcher(doc)\n",
    "\n",
    "for mid, start, end in matches:\n",
    "    print(start, end, doc[start:end])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851adc2b-be4b-4b05-a46b-8774bad29bef",
   "metadata": {},
   "source": [
    "### Operators\n",
    "\n",
    "In the begining of this chapter, we discussed that, the regex gives more nasty codes that why we are moving to spacy. But using some regex function, we will explore spacy more and it will easy our tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ec0b8d-936d-426f-a669-77dca4e70893",
   "metadata": {},
   "source": [
    "<img src=\"images/regex.png\" width=\"600\"/> \n",
    "\n",
    "This are all the kind of operators right, we will use this operators in our specific tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eafb58e8-f530-423c-bea9-34466ecf87be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barack Hussein Obama\n"
     ]
    }
   ],
   "source": [
    "# '?' operator \n",
    "\n",
    "import spacy \n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "doc1 = nlp(\"Barack Obama visited France.\")\n",
    "doc2 = nlp(\"Barack Hussein Obama visited France.\")\n",
    "\n",
    "pattern = [[ \n",
    "    {'LOWER': 'barack'}, \n",
    "    {'LOWER': 'hussein', 'OP': '?'},  \n",
    "    {'LOWER': 'obama'} \n",
    "]] \n",
    "\n",
    "matcher.add('bro', pattern)   \n",
    "matches = matcher(doc2)   # change doc2 to doc1\n",
    "\n",
    "for id, start, end in matches: \n",
    "    print(doc2[start:end])  # change doc2 to doc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ea2385b0-8841-4a35-b3bd-eb512fb50d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aravindan ravi\n"
     ]
    }
   ],
   "source": [
    "NAME_PATTERN = [[   \n",
    "    {'POS': 'PROPN'},\n",
    "    {'POS': 'PROPN', 'OP': '?'}, \n",
    "    {'POS': 'PROPN', 'OP': '?'}\n",
    "]]\n",
    "\n",
    "doc = nlp(\"My name is aravindan ravi\") \n",
    "\n",
    "matcher.add('names', NAME_PATTERN) \n",
    "matches = matcher(doc) \n",
    "\n",
    "\n",
    "mid, start, end = matches[1]  # change change 1 or 2 \n",
    "print(doc[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "099401e2-5e73-409c-a455-d4d7f70e89c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4 Hello hello hello,\n",
      "1 4 hello hello,\n",
      "2 4 hello,\n",
      "3 4 ,\n",
      "7 8 ?\n"
     ]
    }
   ],
   "source": [
    "# * operator (0 or more) \n",
    "\n",
    "doc1 = nlp(\"Hello hello hello, how are you?\")\n",
    "doc2 = nlp(\"Hello, how are you?\")\n",
    "doc3 = nlp(\"How are you?\")\n",
    "\n",
    "pattern = [\n",
    "    {\"LOWER\": {\"IN\": [\"hello\", \"hi\", \"hallo\"]}, \"OP\": \"*\"}, {\"IS_PUNCT\": True}  # you can change * or + \n",
    "]\n",
    "matcher.add(\"greetings\", [pattern])\n",
    "for mid, start, end in matcher(doc1):\n",
    "    print(start, end, doc1[start:end])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6cffc9-e528-4e3a-9ec6-374b0a777b3c",
   "metadata": {},
   "source": [
    "### Using POS\n",
    "\n",
    "Using POS with the matcher, using pos with matcher helps a lot and it will helps a lot. let's see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cd9bc257-6ed2-44eb-a60f-ba6140ddd094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 4 name is Alice\n",
      "3 4 Alice\n",
      "6 9 name was Elliot\n",
      "8 9 Elliot\n",
      "9 10 .\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"My name is Alice and his name was Elliot.\")\n",
    "\n",
    "pattern = [{\"LOWER\": \"name\"},{\"LEMMA\": \"be\"},{}]  # lemma: be -> it captures is, was, be tokens \n",
    "                                                  # {} -> it means wildcard, it captures after 1 token (it may be anything) \n",
    "matcher.add(\"pickName\", [pattern])\n",
    "for mid, start, end in matcher(doc):\n",
    "    print(start, end, doc[start:end])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c77906-1f3f-44c4-9a82-03088c9e2f0d",
   "metadata": {},
   "source": [
    "You can use the wildcard token, when you want to ignore the token or you don't know what specific token it may be: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fcc1c942-747e-4096-9cbd-2cfa18a04881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 4 forwarded his email\n",
      "6 7 .\n",
      "1 4 forwarded an email\n",
      "6 7 .\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(\"I forwarded his email to you.\")\n",
    "doc2 = nlp(\"I forwarded an email to you.\")\n",
    "doc3 = nlp(\"I forwarded the email to you.\")\n",
    "\n",
    "pattern = [{\"LEMMA\": \"forward\"}, {}, {\"LOWER\": \"email\"}]   # now we are including any token that comes inside the two tokens\n",
    "matcher.add(\"forwardMail\", [pattern])\n",
    "\n",
    "for mid, start, end in matcher(doc1):  \n",
    "    print(start, end, doc1[start:end])\n",
    "    \n",
    "for mid, start, end in matcher(doc2):  # try to change 3 \n",
    "    print(start, end, doc2[start:end])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166e1fef-0f90-4b5b-a057-8cf3e658f691",
   "metadata": {},
   "source": [
    "### POS with regex "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4f9935fc-448b-4c79-9d26-94ff0c3769a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2 I travelled\n",
      "4 5 .\n",
      "0 2 She traveled\n",
      "4 5 .\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(\"I travelled by bus.\")\n",
    "doc2 = nlp(\"She traveled by bike.\")\n",
    "\n",
    "pattern = [{\"POS\": \"PRON\"}, {\"TEXT\": {\"REGEX\": \"[Tt]ravell?ed\"}}]  # you used PRON -> possesive pro-noun \n",
    "matcher.add('bro', [pattern])\n",
    "\n",
    "for mid, start, end in matcher(doc1):\n",
    "    print(start, end, doc1[start:end])\n",
    "\n",
    "for mid, start, end in matcher(doc2):\n",
    "    print(start, end, doc2[start:end])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490a7ae6-e67c-42b2-acd7-9e15e11e1267",
   "metadata": {},
   "source": [
    "```Python \n",
    "pattern = [{\"POS\": \"PRON\"}, {\"TEXT\": {\"REGEX\": \"[Tt]ravell?ed\"}}]  # [Tt] -> the first word will be \"T\" or 't'\n",
    "matcher.add('bro', [pattern])                                      # ravell? -> it just a optinal one  \n",
    "                                                                   # ed -> compulsary one \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1c8623b-bcda-456b-9946-06e6e927551a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 went\n",
      "6 7 has\n",
      "7 8 been\n",
      "14 15 has\n",
      "15 16 told\n",
      "18 19 wants\n",
      "20 21 visit\n"
     ]
    }
   ],
   "source": [
    "# Here you are using the verbs \n",
    "\n",
    "import spacy \n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "doc = nlp(\"I went to Italy; he has been there too. His mother also has told me she wants to visit Rome.\")\n",
    "\n",
    "pattern = [{\"TAG\": {\"REGEX\": \"^V\"}}]  # verbs dude\n",
    "matcher.add(\"verbs\", [pattern])\n",
    "\n",
    "for mid, start, end in matcher(doc):\n",
    "    print(start, end, doc[start:end])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c5dbe2-f92b-4a6f-a0e7-205ce75ee3c3",
   "metadata": {},
   "source": [
    "```Python\n",
    "pattern = [{\"TAG\": {\"REGEX\": \"^V\"}}]  # ^V -> (includes all) VB, VGD, VBG, VBN, VBP, VBZ\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212ea101-f466-47ee-911d-d668b2baeacc",
   "metadata": {},
   "source": [
    "If you have any doubt in regex you can use this [**website**](https://regex101.com/) to know what happening inside the regex\n",
    "\n",
    "Same like, if you have any doubt in matcher you can visit this [**website**](https://explosion.ai/demos/matcher) to know what happening inside this. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef8187e-5f67-4081-86c2-873d08bcb7d4",
   "metadata": {},
   "source": [
    "## Phrase Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0373559-95b5-47c8-bcb3-2aba73be9950",
   "metadata": {},
   "source": [
    "While processing financial, medical, or legal text, often we have long lists\n",
    "and dictionaries and we want to scan the text against our lists. As we saw in\n",
    "the previous section, Matcher patterns are quite handcrafted; we coded each\n",
    "token individually. If you have a long list of phrases, Matcher is not very\n",
    "handy. It's not possible to code all the terms one by one.\n",
    "spaCy offers a solution for comparing text against long dictionaries ‚Äì the\n",
    "PhraseMatcher class. The PhraseMatcher class helps us match long\n",
    "dictionaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b6b707-f08c-4f2e-b159-8ebefa91904a",
   "metadata": {},
   "source": [
    "```Python \n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher  # phrase matcher class\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "matcher = PhraseMatcher(nlp.vocab)  # initialized phrase matcher object with nlp.vocab\n",
    "\n",
    "terms = [\"Angela Merkel\", \"Donald Trump\", \"Alexis Tsipras\"]\n",
    "patterns = [nlp.make_doc(term) for term in terms]  # make_doc helps to create one by one patterns, In another term it will create a doc for individual word and acts like a tokenizer.\n",
    "\n",
    "matcher.add(\"politiciansList\", None, *patterns)  \n",
    "\n",
    "doc = nlp(\"3 EU leaders met in Berlin. German chancellor Angela Merkel first welcomed the US president Donald Trump. The following day Alexis Tsipras joined them in Brandenburg.\")\n",
    "\n",
    "matches = matcher(doc)\n",
    "for mid, start, end in matches:\n",
    "    print(start, end, doc[start:end])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db2c14b7-2ea7-48b5-8508-a87b10d4270a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 11 Angela Merkel\n",
      "16 18 Donald Trump\n",
      "22 24 Alexis Tsipras\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "terms = [\"Angela Merkel\", \"Donald Trump\", \"Alexis Tsipras\"]\n",
    "patterns = [nlp.make_doc(term) for term in terms]\n",
    "matcher.add(\"politiciansList\", None, *patterns)\n",
    "\n",
    "doc = nlp(\"3 EU leaders met in Berlin. German chancellor Angela Merkel first welcomed the US president Donald Trump. The following day Alexis Tsipras joined them in Brandenburg.\")\n",
    "matches = matcher(doc)\n",
    "\n",
    "for mid, start, end in matches:\n",
    "    print(start, end, doc[start:end])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e08b94-4ff3-4cee-bb53-1d4c838ffba5",
   "metadata": {},
   "source": [
    "Using Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cb5e0e2-2d17-4c5e-9fbc-0065a684cdf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 6 derivatives\n",
      "6 7 market\n",
      "9 10 asset\n"
     ]
    }
   ],
   "source": [
    "matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")  # Lower attribute\n",
    "\n",
    "terms = [\"Asset\", \"Investment\", \"Derivatives\", \"Demand\", \"Market\"]\n",
    "patterns = [nlp.make_doc(i) for i in terms] \n",
    "\n",
    "matcher.add('Summa', None, *patterns)\n",
    "\n",
    "doc = nlp(\"During the last decade, derivatives market became an asset class of their own and influenced the financial landscape strongly.\")\n",
    "matches = matcher(doc) \n",
    "\n",
    "for id, start, end in matches: \n",
    "    print(start, end, doc[start:end])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72165a81-ff1d-457e-8d5c-5cac02c3553f",
   "metadata": {},
   "source": [
    "If you want to extract the system logs, IP numbers, dates and other numerical values, we will use the Matcher object with shape, it helps to extract the information from the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b7569c2-09b6-460b-b085-48746432521c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 9 192.1.1.1\n",
      "12 13 192.160.1.1\n"
     ]
    }
   ],
   "source": [
    "matcher = PhraseMatcher(nlp.vocab, attr=\"SHAPE\")  # shape attribute\n",
    "\n",
    "ip_nums = [\"127.0.0.0\", \"127.256.0.0\"]\n",
    "patterns = [nlp.make_doc(ip) for ip in ip_nums]\n",
    "matcher.add(\"IPNums\", None, *patterns)\n",
    "\n",
    "doc = nlp(\"This log contains the following IP addresses: 192.1.1.1 and 192.12.1.1 and 192.160.1.1 .\")\n",
    "for mid, start, end in matcher(doc):\n",
    "    print(start, end, doc[start:end])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543b823f-6c24-4e4d-b500-7261b6bbbe67",
   "metadata": {},
   "source": [
    "## Entity Ruler\n",
    "\n",
    "We know entity is used to map person, place, things. The awesome thing is that, we can extract the person, name (entities) using this entity ruler. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1f818a4a-f2fa-44db-ad91-e02e243d554b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aravind\n",
      "Bill\n",
      "Gates\n"
     ]
    }
   ],
   "source": [
    "pattern = [{\"ENT_TYPE\": \"PERSON\"}]  # we are mentioning entity type as person, let's see the magic \n",
    "\n",
    "matcher.add(\"personEnt\", [pattern]) \n",
    "\n",
    "doc = nlp(\"Aravind loves Bill Gates too much\")\n",
    "\n",
    "matches = matcher(doc) \n",
    "\n",
    "for id_, start, end in matches: \n",
    "    print(doc[start:end]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e05f17-3485-4e0d-b3d1-0d4e666a2849",
   "metadata": {},
   "source": [
    "Here we got indivial names, if you want to extract full names, you can use **Operator** in the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9ad747ff-bb49-4f9f-b5b8-26386ebafe55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aravind\n",
      "Ravi\n",
      "Aravind Ravi\n",
      "Bill\n",
      "Gates\n",
      "Bill Gates\n"
     ]
    }
   ],
   "source": [
    "pattern = [{\"ENT_TYPE\": \"PERSON\", \"OP\":\"+\"}]  # we are mentioning entity type as person, let's see the magic \n",
    "\n",
    "matcher.add(\"personEnt\", [pattern]) \n",
    "\n",
    "doc = nlp(\"Aravind Ravi loves Bill Gates too much\")\n",
    "\n",
    "matches = matcher(doc) \n",
    "\n",
    "for id_, start, end in matches: \n",
    "    print(doc[start:end]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98425c7a-106d-4b9a-b851-176798bf2f80",
   "metadata": {},
   "source": [
    "Usually, we do this with lots of operators and pos (I mean, we extract lots of information in a line), let's see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c18223ba-65bf-4139-845b-6f54239cb34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 6 Merkel met\n",
      "3 6 Angela Merkel met\n"
     ]
    }
   ],
   "source": [
    "import spacy \n",
    "from spacy.matcher import Matcher \n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "pattern = [[\n",
    "    {\"ENT_TYPE\":\"PERSON\", \"OP\":\"+\"}, \n",
    "    {\"POS\":\"VERB\"} \n",
    "]]\n",
    "\n",
    "matcher.add('podu', pattern) \n",
    "doc = nlp(\"Today German chancellor Angela Merkel met with the US president.\")\n",
    "\n",
    "matches = matcher(doc) \n",
    "for mid, start, end in matches: \n",
    "    print(start, end, doc[start:end])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00823fc2-4da7-424b-97b0-22376f806484",
   "metadata": {},
   "source": [
    "See, we get the verb after pron, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9a81fa-1cc3-4e37-8313-b3a01ae5e0d2",
   "metadata": {},
   "source": [
    "## Combining  spaCy models and matchers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a737413-be84-42d3-a74d-9e94d4bd34fc",
   "metadata": {},
   "source": [
    "Let's talk about the domain releated problems. like you want to extract the greek names from the wikipedia aritcle, our normal spacy model fails to do that, because spaCy trained in english words so it don't extract the  words, like this cases you can use the entity ruler. \n",
    "\n",
    "The entity ruler is not a class, it's just a pipeline, like if you want to add new to entites in the pipeline, you can add the ent using this entity ruler. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "808967b2-55a4-41db-8348-7741119040aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treblinka GPE\n",
      "Poland GPE\n",
      "Treblinka GPE\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "text = \"The village of Treblinka is in Poland. Treblinka was also an extermination camp.\"\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")  # add entity ruler to our spacy pipeline \n",
    "\n",
    "patterns = [{\"label\": \"GPE\", \"pattern\": \"Treblinka\"}]\n",
    "ruler.add_patterns(patterns)  # add patterns to the entity ruler pipeline \n",
    "doc = nlp(text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print (ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7519ae45-74ad-421b-a77a-11b6c924af1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2017,)\n",
      "see you have only date here bellow this you have a chime, it was added by entity ruler\n",
      "chime ORG\n",
      "2017 DATE\n"
     ]
    }
   ],
   "source": [
    "import spacy \n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "doc = nlp(\"I have an acccount with chime since 2017\")\n",
    "print(doc.ents)\n",
    "print('see you have only date here', 'bellow this you have a chime, it was added by entity ruler')\n",
    "\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")  # create pipeline \n",
    "\n",
    "patterns = [{\"label\": \"ORG\", \"pattern\": [{\"LOWER\": \"chime\"}]}]  # create patterns \n",
    "ruler.add_patterns(patterns)  # add your patterns\n",
    "\n",
    "doc = nlp(\"I have an acccount with chime since 2017\")\n",
    "for ent in doc.ents: \n",
    "    print (ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f627e3-f273-4dac-9f00-453ef32f5d6b",
   "metadata": {},
   "source": [
    "This is really an amazing features, we can combine both the matcher and entity ruler to do lot's of things, let's go for real world examples and parse out the IBAN numbers in the list. IBAN numbers are just a finanical numbers that looks like this `BE71 0961 2345 6769`, `BE71 ME61 2345 6769`\n",
    "\n",
    "We can able to say that, the IBAN numbers are starting with two caps letters following by the two numbers, so we use `{'SHAPE':\"XXdd\"}` Here the XX-> represents two capital letters and dd-> represents two numbers. \n",
    "\n",
    "Then we have sequence of 4 numbers for three blocks and sometiems the second block number is starting from the two caps digit so, we use regular expressiion for this  `\\d{1,4}` - > This pattern will match the digits block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0ba6f67b-7084-4cf9-98f1-18400ab04dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BE71 0961\n",
      "BE71 0961 2345\n",
      "BE71 0961 2345 6769\n",
      "FR76 BE00\n",
      "FR76 BE00 6000\n",
      "BE00 6000\n",
      "FR76 BE00 6000 0112\n",
      "BE00 6000 0112\n",
      "FR76 BE00 6000 0112 3456\n",
      "BE00 6000 0112 3456\n",
      "FR76 BE00 6000 0112 3456 7890\n",
      "BE00 6000 0112 3456 7890\n",
      "FR76 BE00 6000 0112 3456 7890 189\n",
      "BE00 6000 0112 3456 7890 189\n"
     ]
    }
   ],
   "source": [
    "import spacy \n",
    "import en_core_web_md\n",
    "from spacy.matcher import Matcher\n",
    "nlp = en_core_web_md.load()\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "pattern = [[ \n",
    "    {\"SHAPE\": \"XXdd\"}, \n",
    "    {\"TEXT\": {\"REGEX\": \"\\d{1,4}\"}, \"OP\":\"+\"}\n",
    "]]\n",
    "\n",
    "doc1 = nlp(\"My IBAN number is BE71 0961 2345 6769, please send the money there.\")\n",
    "doc2 = nlp(\"My IBAN number is FR76 BE00 6000 0112 3456 7890 189 please send the money there.\")  # STARTING WITH DIGITS \n",
    "\n",
    "matcher.add('summa', pattern) \n",
    "matches1 = matcher(doc1)\n",
    "matches2 = matcher(doc2)\n",
    "\n",
    "for id_, start, end in matches1:\n",
    "    print(doc1[start:end]) \n",
    "    \n",
    "for id_, start, end in matches2:\n",
    "    print(doc2[start:end]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8114598-206e-4749-bc86-84f67e3f2b2c",
   "metadata": {},
   "source": [
    "let's do how to extract the phone numbers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47baa755-55cc-486a-9e99-92f8c7f1fddd",
   "metadata": {},
   "source": [
    "Let's start with the US phone number format. A US number is written as (541)\n",
    "754-3010 domestically or +1 (541) 754-3010 internationally. We can form\n",
    "our pattern with an optional +1, then a three-digit area code, then two blocks\n",
    "of numbers separated with an optional -. \n",
    "\n",
    "Here is the pattern:\n",
    "\n",
    "```{\"TEXT\": \"+1\", \"OP\": \"?\"}, {\"TEXT\": \"(\"}, {\"SHAPE\": \"ddd\"},\n",
    "{\"TEXT\": \")\"}, {\"SHAPE\": \"ddd\"}, {\"TEXT\": \"-\", \"OP\": \"?\"},\n",
    "{\"SHAPE\": \"dddd\"}```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec4bc5a5-0d0d-44cb-bcc5-78349936cde9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+1 (221) 102-2423\n",
      "(221) 102-2423\n",
      "(221) 102 2423\n"
     ]
    }
   ],
   "source": [
    "import spacy \n",
    "from spacy.matcher import Matcher \n",
    "\n",
    "nlp = en_core_web_md.load()\n",
    "matcher = Matcher(nlp.vocab) \n",
    "\n",
    "pattern = [[ \n",
    "    {\"TEXT\": \"+1\", \"OP\": \"?\"}, {\"TEXT\": \"(\"},\n",
    "    {\"SHAPE\": \"ddd\"}, {\"TEXT\": \")\"},\n",
    "    {\"SHAPE\": \"ddd\"}, {\"TEXT\": \"-\", \"OP\": \"?\"},\n",
    "    {\"SHAPE\": \"dddd\"}\n",
    "]] \n",
    "\n",
    "doc1 = nlp(\"You can call my office on +1 (221) 102-2423 or email me directly.\")\n",
    "doc2 = nlp(\"You can call me on (221) 102 2423 or text me.\")\n",
    "\n",
    "matcher.add('summa1', pattern) \n",
    "\n",
    "matches1 = matcher(doc1) \n",
    "matches2 = matcher(doc2) \n",
    "\n",
    "for id_, start, end in matches1: \n",
    "    print(doc1[start:end])\n",
    "    \n",
    "for id_, start, end in matches2: \n",
    "    print(doc2[start:end])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16cc321-d862-4cf2-9aee-ff07dfe864d5",
   "metadata": {},
   "source": [
    "Let's try with `indian phone number`. The phone number looks like this `+91 9786420128`, `9786420128`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6f9b65be-766a-4fdb-a71d-52d0ecb0e8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+91 9786420128\n",
      "9786420128\n"
     ]
    }
   ],
   "source": [
    "pattern = [[ \n",
    "    {\"TEXT\":\"+91\", \"OP\":\"?\"}, \n",
    "    {\"TEXT\": {\"REGEX\":\"\\d{10}\"}, \"OP\":\"+\"}\n",
    "]]\n",
    "\n",
    "matcher.add('summa2', pattern) \n",
    "doc1 = nlp(\"You can call my office on +91 9786420128 or email me directly.\")\n",
    "\n",
    "matches1 = matcher(doc1) \n",
    "\n",
    "for id_, start, end in matches1: \n",
    "    print(doc1[start:end])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b460ab-6571-4d45-8e13-3b26d2fdc925",
   "metadata": {},
   "source": [
    "Let's extract the mentions, what I mean is nothing but extracting the  companies are mentioned in which ways. by using this small dataset\n",
    "\n",
    "```Markdown\n",
    "CafeA is very generous with the portions.\n",
    "CafeB is horrible, we waited for mins for a table.\n",
    "RestaurantA is terribly expensive, stay away!\n",
    "RestaurantB is pretty amazing, we recommend.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd5cc0b-3be3-4737-a356-36c2bd80c298",
   "metadata": {},
   "source": [
    "What we're looking for is most probably patterns of the BusinessName\n",
    "is/was/be adverb* adjective form. The following pattern would work:\n",
    "\n",
    "`pattern = [{\"ENT_TYPE\": \"ORG\"}, {\"LEMMA\": \"be\"}, {\"POS\": \"ADV\", \"OP\": \"*\"}, {\"POS\": \"ADJ\"}]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "db0e87e7-34cc-44e1-ac86-316d38ea8287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "k\n"
     ]
    }
   ],
   "source": [
    "import spacy \n",
    "from spacy.matcher import Matcher \n",
    "\n",
    "nlp = en_core_web_md.load()\n",
    "matcher = Matcher(nlp.vocab) \n",
    "\n",
    "pattern = [{\"LOWER\": \"acme\"}, {\"LEMMA\": \"be\"}, {\"POS\": \"ADV\", \"OP\": \"*\"}, {\"POS\": \"ADJ\"}]\n",
    "\n",
    "\n",
    "doc = nlp(\"CafeA is very generous with the portions. CafeB is horrible, we waited for mins for a table. RestaurantA is terribly expensive, stay away! RestaurantB is pretty amazing, we recommend.\")\n",
    "\n",
    "matcher.add('summa3', [pattern]) \n",
    "\n",
    "matches = matcher(doc) \n",
    "\n",
    "for id_, start, end in matches: \n",
    "    print(doc[start:end])\n",
    "print(matches)\n",
    "print('k')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd46931-c31c-4e9c-baf4-d775b47956fa",
   "metadata": {},
   "source": [
    "I don't know why we are not getting this. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986dcf7b-0222-4b37-a992-43e762d41af4",
   "metadata": {},
   "source": [
    "**Let's match hastag**\n",
    "\n",
    "Hastag has no space and '#\" after this there is no space,so we can get easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cb271d8a-5772-4b7e-8f93-550cdc293e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 6 #WeekendShred\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Start working out now #WeekendShred\")\n",
    "\n",
    "pattern = [{\"TEXT\": \"#\"}, {\"IS_ASCII\": True}]\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"hashTag\", [pattern])\n",
    "matches = matcher(doc)\n",
    "\n",
    "for mid, start, end in matches:\n",
    "    print(start, end, doc[start:end])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59263ce4-562f-40e9-b3ea-1a8e9c8f874c",
   "metadata": {},
   "source": [
    "**Let's match emoji**\n",
    "\n",
    "An emoji is usually coded with lists according to their\n",
    "sentiment value, such as positive, negative, happy, sad, and so on. Here,\n",
    "we'll separate emojis into two classes, positive and negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b0531cb8-19c9-4c48-8a4e-3fc4853d552c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 5 üòø\n"
     ]
    }
   ],
   "source": [
    "import spacy \n",
    "from spacy.matcher import Matcher \n",
    "\n",
    "nlp = en_core_web_md.load()\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "pos_emoji = [\"üòÜ\", \"üôÇ\", \"üòé\"]\n",
    "neg_emoji = [\"üò≠\", \"üíî\", \"üòø\"]\n",
    "\n",
    "\n",
    "pos_patterns = [[{\"ORTH\": emoji}] for emoji in pos_emoji]\n",
    "neg_patterns = [[{\"ORTH\": emoji}] for emoji in neg_emoji]\n",
    "\n",
    "matcher.add(\"posEmoji\", pos_patterns)\n",
    "matcher.add(\"negEmoji\", neg_patterns)\n",
    "\n",
    "\n",
    "doc = nlp(\" I love Zara üòø\")\n",
    "for mid, start, end in matcher(doc):\n",
    "    print(start, end, doc[start:end])\n",
    "    # print(doc[start:end].name_)  # Here I need to get the pattern name \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5fae8a0c-4ed8-4461-919a-e87f1de13ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Ms.', 'TITLE'), ('Smith', 'PERSON')]\n"
     ]
    }
   ],
   "source": [
    "# let's extract mr woman and mr men or prof doctor \n",
    "\n",
    "import spacy \n",
    "nlp = en_core_web_md.load()\n",
    "\n",
    "patterns = [{\"label\": \"TITLE\", \"pattern\": [{\"LOWER\": {\"IN\": [\"ms.\", \"mr.\", \"mrs.\", \"prof.\", \"dr.\"]}}]}]\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "doc = nlp(\"Ms. Smith left her house\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
