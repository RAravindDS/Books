{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1feca0ea-17f5-4a0d-b559-60185a077d28",
   "metadata": {},
   "source": [
    "**In chapter, we are going to learn this thins**. \n",
    "\n",
    "Core Operation of spaCy, such as creating a pipeline, tokenization, text into senteces. \n",
    "\n",
    "\n",
    "The chapter code can be found at the book's GitHub [**repository:**](https://github.com/PacktPublishing/Mastering-spaCy/tree/main/Chapter02)\n",
    "\n",
    "\n",
    "Every NLP applications consists of several steps of pre-processing steps. We called nlp and doc instances in the previous chatper,Nlp for loading the language model.and inside the doc we specify the text, that will do some pre-processig steps. The first step is create a tokenizer to create a doc object. The doc object is processed further with a tagger, parser, and ner. This way of processing is called **language processing pipeline**.  \n",
    "\n",
    "```Python\n",
    "import spacy \n",
    "nlp = spacy.load(en_core_web_sm)  # first instances \n",
    "doc = nlp('I went there')  # doc -> second instances \n",
    "\n",
    "# We started by importing spaCy.\n",
    "# In the second line, spacy.load() returned a Language class instance, nlp The Language class is the text processing pipeline.\n",
    "# After that, we applied nlp on the sample sentence I went there and got a Doc class instance, doc.\n",
    "``` \n",
    "<!-- ![img](images/spacy_pipeline.png) -->\n",
    "<img src=\"images/spacy_pipeline.png\" width=\"600\"/>\n",
    "\n",
    "The sample text after passing to the nlp class, it does the pipeline for us and the doc instances will help to execute the tagger, ner and parser. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268968f0-5e2c-4c82-acd3-ee42f9bd3587",
   "metadata": {},
   "source": [
    "<!-- ![pip](images/processing_pipeline.png) -->\n",
    "<img src=\"images/processing_pipeline.png\" width=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a969cc-fb80-4b20-a372-ba0321d47eff",
   "metadata": {},
   "source": [
    "Container Classes such as **Doc** helps to hold the information such as sentences, words and text. There are also container classes other than DOC: \n",
    "\n",
    "* DOC -> A container of linguistic features. \n",
    "* Span -> A slice of DOC. \n",
    "* Token -> A individual token. \n",
    "* Lexeme -> An entry in the vocabulary. It's a word type with no context, as opposed to a word token. therfore no pre-processsing steps here. \n",
    "* Vocab -> A lookup table for vocabulary that allows to access lexeme objects. \n",
    "* Vectors -> Contains vectors. \n",
    "* and more..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716ed297-5a6b-428c-9ad1-b360a3319207",
   "metadata": {},
   "source": [
    "The spaCy library's backbone data structures are Doc and Vocab. The Doc\n",
    "object abstracts the text by owning the sequence of tokens and all their\n",
    "properties. The Vocab object provides a centralized set of strings and lexical\n",
    "attributes to all the other classes. \n",
    "\n",
    "\n",
    "### Arcitecture of spaCy. \n",
    "\n",
    "<!-- ![img](images/spacy_archi.png) -->\n",
    "<img src=\"images/spacy_archi.png\" width=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d5b831-e5c5-486f-a278-fbffd334f715",
   "metadata": {},
   "source": [
    "### Tokenizer \n",
    "\n",
    "Tokenizer is the first step of the all the process, All the future pre-processsing excepts the token as the input. Tokenization simply means splitting the sentences into tokens. Tokens can be unit of sematics like small meanigful part of text, it can be word, numbers, punctuationm and currency symbols, moree..\n",
    "\n",
    "* USA, \n",
    "* N.Y, \n",
    "* 33 \n",
    "* ! \n",
    "This are all the examples of tokens. \n",
    "\n",
    "```Python \n",
    "import spacy \n",
    "nlp = spacy.load('en_core_web_md') \n",
    "doc = nlp(\"I own a ginger cat!\") \n",
    "\n",
    "print( [ word.text for word in doc ] ) \n",
    "\n",
    "# Stated wth spacy \n",
    "# Then we loaded the English language model via the en shortcut to create an instance of the nlp Language class.\n",
    "# Next, we apply the nlp object to the input sentence to create a Doc object, doc. A Doc object is a container for a sequence of Token objects. spaCy generates the Token objects implicitly when we created the Doc object\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2507c5c4-bce2-4331-8cda-fca19bd1803f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'own', 'a', 'ginger', 'cat', '!']\n"
     ]
    }
   ],
   "source": [
    "import spacy \n",
    "nlp = spacy.load('en_core_web_md') \n",
    "doc = nlp(\"I own a ginger cat!\") \n",
    "\n",
    "print( [ word.text for word in doc ] ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f827b09-d207-474c-abd7-87788e9402e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', \"'s\", 'been', 'a', 'crazy', 'week', '!', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"It's been a crazy week!!!\")\n",
    "\n",
    "print( [word.text for word in doc ] ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cb072f-cd3c-44db-a4dd-64251184c97a",
   "metadata": {},
   "source": [
    "HOw Spacy model indetify this things very clearly, because when you do this same thing in the `split()` class, you will wrong info. This is because spacy uses lot's of grammtical rules inside (i.e Language specific rules). If you look at this [**spaCy coDe**](https://github.com/explosion/spaCy/blob/master/spacy/lang/en/tokenizer_exceptions.py), you will see that rules look like {ORTH: \"n't\", LEMMA:\n",
    "\"not\"}, which describes the splitting rule for n't to the tokenizer. Here, ORTH means the text and LEMMA means the base word form.\n",
    "\n",
    "### Tokenizer algorithm \n",
    "\n",
    "![algo](images/tokenizer_algo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93e26a8-8a78-423b-9433-48d7f5d4fb51",
   "metadata": {},
   "source": [
    "### Customizing tokenization: \n",
    "Each domain such as medicene, or finance we often come across with specific words.  Most domain that you'll process have characteristic words and phrases that needs custom tokenization rules. \n",
    "\n",
    "```Python \n",
    "import spacy\n",
    "from spacy.symbols import ORTH\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "doc = nlp(\"lemme that\")\n",
    "\n",
    "print([w.text for w in doc])\n",
    "['lemme', 'that']  # output \n",
    "\n",
    "special_case = [{ORTH: \"lem\"}, {ORTH: \"me\"}]  # orthography means text \n",
    "nlp.tokenizer.add_special_case(\"lemme\", special_case)\n",
    "\n",
    "print([w.text for w in nlp(\"lemme that\")])\n",
    "['lem', 'me', 'that']  # output \n",
    "```\n",
    "\n",
    "1. We again started by importing spacy.\n",
    "2. Then, we imported the ORTH symbol, which means orthography; that is,\n",
    "text.\n",
    "3. We continued with creating a Language class object, nlp, and created a\n",
    "Doc object, doc.\n",
    "4. We defined a special case, where the word lemme should tokenize as\n",
    "two tokens, lem and me.\n",
    "5. We added the rule to the nlp object's tokenizer.\n",
    "6. The last line exhibits how the fresh rule works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b25ad06-f9d1-4b4c-b9ad-18a6c9a7bcd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['...lemme...?']\n"
     ]
    }
   ],
   "source": [
    "nlp.tokenizer.add_special_case(\"...lemme...?\", [{\"ORTH\":\"...lemme...?\"}])\n",
    "print([w.text for w in nlp(\"...lemme...?\")])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a786f08-5fe1-4693-9297-28534ba176e6",
   "metadata": {},
   "source": [
    "The difference is that, \"?\" is a unique punctuation but we created our own rule and see the output, it's amazing right? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979ed3cd-143b-4eba-8d75-b087f9e42bfc",
   "metadata": {},
   "source": [
    "### Debugging the tokenizer: \n",
    "* If you want to know what exactly happen inside the tokenizer, it has a debugging tool. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "144d8878-c50f-4b6f-bf4f-d530cf1dd300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let \t SPECIAL-1\n",
      "'s \t SPECIAL-2\n",
      "go \t TOKEN\n",
      "! \t SUFFIX\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "text = \"Let's go!\"\n",
    "doc = nlp(text)\n",
    "\n",
    "tok_exp = nlp.tokenizer.explain(text)\n",
    "\n",
    "for t in tok_exp:  # iterator \n",
    "    print(t[1], \"\\t\", t[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd239bc-725a-46e9-9f17-f87f8e9505bd",
   "metadata": {},
   "source": [
    "### Sentence Segmentation: \n",
    "Breaking a text into sentece is called **Sentence Segmentation**. \n",
    "\n",
    "```Python \n",
    "text = \"I flied to N.Y yesterday. It was around 5 pm.\"\n",
    "doc= nlp(text) \n",
    "\n",
    "sent = [i.text for i in doc.sents] \n",
    "print(sent)\n",
    "\n",
    "I flied to N.Y yesterday. # output \n",
    "It was around 5 pm.  # output \n",
    "```\n",
    "\n",
    "Determining sentence boundaries is a more complicated task than\n",
    "tokenization. As a result, spaCy uses the dependency parser to perform\n",
    "sentence segmentation. This is a unique feature of spaCy â€“ no other library\n",
    "puts such a sophisticated idea into practice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daf2ada-6292-44a4-8096-ce64ea553ae3",
   "metadata": {},
   "source": [
    "### Lemmatization: \n",
    "Lemmatization is the process reducing the word forms into lemma. Lemmas means the base form of the token like `eating` the lemma(base form) `eat`\n",
    "\n",
    "```Python \n",
    "oc = nlp(\"I went there for working and worked for 3 years.\")\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66cf97d4-a5d4-45b9-95bf-e031dfbbd492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: I\t, Lemma: I\n",
      "Tokens: went\t, Lemma: go\n",
      "Tokens: there\t, Lemma: there\n",
      "Tokens: for\t, Lemma: for\n",
      "Tokens: working\t, Lemma: working\n",
      "Tokens: and\t, Lemma: and\n",
      "Tokens: worked\t, Lemma: work\n",
      "Tokens: for\t, Lemma: for\n",
      "Tokens: 3\t, Lemma: 3\n",
      "Tokens: years\t, Lemma: year\n",
      "Tokens: .\t, Lemma: .\n",
      "Tokens: breaking\t, Lemma: break\n",
      "Tokens: all\t, Lemma: all\n",
      "Tokens: over\t, Lemma: over\n",
      "Tokens: the\t, Lemma: the\n",
      "Tokens: coutries\t, Lemma: coutrie\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I went there for working and worked for 3 years. breaking all over the coutries\")\n",
    "for token in doc:\n",
    "    print(f'Tokens: {token.text}\\t, Lemma: {token.lemma_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd314c16-574f-407c-b457-2ffcd7f61f1e",
   "metadata": {},
   "source": [
    "Reminder:\n",
    "\n",
    "`orth` -> orthography (text), `lemma` -> base form of tokens(meaningful). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71a6f0d-ccad-4465-9f4e-071725859edb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Stemming: \n",
    "\n",
    "Stemming is the poor form of the tokens(lemma_), it just cut down the prefix and suffix of a word, the stem word does not have a meaningful at all. The stemming algorithm just cut down the head and tail based on some hand made rules. And lemma contains in the dictionary and stem don't. Stem don't follow any grammtical protocols. But even though the word structure is not good but it still used in NLU (natural language understanding) \n",
    "\n",
    "\n",
    "Stemming gives very good\n",
    "results if you apply only statistical algorithms to the text, without further semantic processing\n",
    "such as pattern lookup, entity extraction, coreference resolution, and so on. Also stemming\n",
    "can trim a big corpus to a more moderate size and give you a compact representation. If you\n",
    "also use linguistic features in your pipeline or make a keyword search, include lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4db730-737e-4205-b4b7-724b289477e2",
   "metadata": {},
   "source": [
    "### Container -Briefly:\n",
    "\n",
    "Container means we already sawed like DOC, TOKEN, LEXEME, and SPAN. \n",
    "\n",
    "By using this container objects we can access the linguistic properties that spaCy assings to the text. \n",
    "\n",
    "A container is the logical representation of the text units such as token, document, slice. \n",
    "\n",
    "Container objects in spaCy follow the natural structure of the text: a\n",
    "document is composed of sentences and sentences are composed of tokens. \n",
    "\n",
    "#### Let's see about doc \n",
    "* Doc one type of container. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "90a432f2-091e-42b1-bc46-e90d27d4ad11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like Cats.\n",
      "['I', 'like', 'Cats', '.']\n",
      "like\n",
      "4\n",
      "<generator object at 0x000001B7D149DD60>\n",
      "[This is a sentence., This is the second sentence]\n",
      "(second,)\n",
      "[Sweet brown fox, the fence]\n",
      "en\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm') \n",
    "\n",
    "doc = nlp(\"I like Cats.\") \n",
    "print(doc.text)  # print the unicode text \n",
    "\n",
    "\n",
    "# Then the second step is tokens \n",
    "print([ i.text for i in doc ] )  # returns a tokens \n",
    "print(doc[1])  # you can also retrive tokens using indexing \n",
    "print(len(doc)) \n",
    "\n",
    "# Then the thirs is text to sentences \n",
    "doc = nlp(\"This is a sentence. This is the second sentence\")\n",
    "print(doc.sents)  # it is an iterator \n",
    "sentences = list(doc.sents) \n",
    "print(sentences)\n",
    "\n",
    "\n",
    "# Then we can print the NER also \n",
    "print(doc.ents,)\n",
    "\n",
    "\n",
    "# Then we can print the noun chunks also (noun pharses found in the text) \n",
    "doc = nlp(\"Sweet brown fox jumped over the fence.\")\n",
    "print(list(doc.noun_chunks))\n",
    "\n",
    "\n",
    "# Then we can print the language also \n",
    "print(doc.lang_)\n",
    "\n",
    "\n",
    "# Then we can convert them to json \n",
    "# doc.to_json() # run if you want"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac72c29-bb9e-46a3-9eb5-d810aff14d59",
   "metadata": {},
   "source": [
    "### Token\n",
    "A token object represent a word, Tokens can be unit of sematics like small meanigful part of text, it can be word, numbers, punctuationm and currency symbols, and more. This is the building block of the span and doc container objects. Token are **class**. We usually don't construct the token object ratherly we will take the token from doc object. \n",
    "\n",
    "```Python \n",
    "import spacy \n",
    "nlp = spacy.load('en_core_web_md') \n",
    "\n",
    "doc = nlp('I am a good boy') \n",
    "print(doc[0].text)  # text-> tokenizer  \n",
    "\n",
    "'I'  # output \n",
    "\n",
    "\n",
    "print(doc[0].text_with_ws)  # it will give with the whitespace if the word is having \n",
    "\n",
    "'I '  # output \n",
    "\n",
    "token = doc[2] \n",
    "print(token.i)  # i -> will give the index of the token \n",
    "\n",
    "doc = nlp(\"He entered the room. Then he nodded.\")\n",
    "doc[0].is_sent_start  # it will tell the doc[0] starting in the line or not \n",
    "\n",
    "'True'  # output \n",
    "```\n",
    "\n",
    "Tokens are very usefull to find the lemma in our word \n",
    "\n",
    "``` Python \n",
    "doc = nlp(\"I went there.\")\n",
    "doc[1].lemma_\n",
    "\n",
    "'go'  # output \n",
    "```\n",
    "\n",
    "We can able to the get the NER from the tokens only~\n",
    "\n",
    "```Python \n",
    "doc = nlp(\"President Trump visited Mexico City.\")\n",
    "doc.ents\n",
    "\n",
    "Trump, Mexico city  # output \n",
    "\n",
    "doc[1].ent_type_  # It will give labels for your ent\n",
    "\n",
    "'Person'  # ent \n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70f15868-0823-45ad-afcc-1f9a595fe3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "I \n",
      "0\n",
      "True\n",
      "go\n",
      "(Trump, Mexico City)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'PERSON'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy \n",
    "nlp = spacy.load('en_core_web_sm') \n",
    "\n",
    "doc = nlp(\"I am a good boy\") \n",
    "print(doc[0].text)\n",
    "\n",
    "print(doc[0].text_with_ws) \n",
    "\n",
    "print(doc[0].i) \n",
    "\n",
    "doc = nlp(\"He entered the room. Then he nodded.\")\n",
    "print(doc[0].is_sent_start)\n",
    "\n",
    "\n",
    "doc = nlp(\"I went there.\")\n",
    "print(doc[1].lemma_)\n",
    "\n",
    "doc = nlp(\"President Trump visited Mexico City.\")\n",
    "print(doc.ents)\n",
    "\n",
    "doc[1].ent_type_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7854aeb6-14d4-43ca-b218-f32e0983e414",
   "metadata": {},
   "source": [
    "<h2 style=\"background-color:cyan\">\n",
    "Note you need not to remember this, if you have doubt just do dir(token) to get all this features with documentation \n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c66297-b508-4fd9-8a65-30684a68e3b1",
   "metadata": {},
   "source": [
    "## Span \n",
    "\n",
    "Span objects represent a pharses or segments of the text. Span has contiguous sequence of words. we use the span by doc object. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ffa23c7-a68e-4b35-b76e-a5bf2545e2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love Atlanta\n",
      "None\n",
      "2\n",
      "6\n",
      "9\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I know that you have been to USA.\")\n",
    "doc[2:4]\n",
    "\n",
    "doc = nlp(\"You love Atlanta since you're 20.\")\n",
    "print(doc.char_span(4, 16))\n",
    "\n",
    "\n",
    "doc = nlp(\"You went there after you saw me\")\n",
    "span = doc[2:6]\n",
    "print(span.char_span(15,24))\n",
    "\n",
    "span = doc[2:6] \n",
    "print(span.start) \n",
    "print(span.end) \n",
    "print(span.start_char) \n",
    "print(span.end_char) \n",
    "\n",
    "small_doc = span.as_doc()  # it will convert your span to doc, Now you can user ents, sent, text, text_with_ws and more doc characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3776cf7c-0a2f-4c1e-89fb-ed15ef8e30eb",
   "metadata": {},
   "source": [
    "## More spaCy feaTures: \n",
    "Spacy contains the lot of features, lets discuss the few popular features of the spaCy. Most of the time we will eliminate un usuall word like links, email, digits and more. Let's see how to find that inside the doc object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c361f4ce-06ba-4fb0-87b0-8cd1a3a9c0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "hi i am aravind hi have $1 billion dollars in my bank and i have 82 roab bikes with ' ( > < me!\n",
      "HI I AM ARAVIND HI HAVE $1 BILLION DOLLARS IN MY BANK AND I HAVE 82 ROAB BIKES WITH ' ( > < ME!\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"HI i am aravind hi have $1 billion dollars in my bank and I have 82 roab bikes with ' ( > < me!\") \n",
    "\n",
    "print(doc[0].lower_)   # you can't lower all by iterating only you can able to lower down all \n",
    "\n",
    "print(doc.text.lower())   # it will help to lower your all sentecnex \n",
    "\n",
    "print(doc.text.upper())   # upper \n",
    "\n",
    "print(doc[3].is_lower)  # is lower help to find the doc word is lower or not \n",
    "\n",
    "print(doc[1].is_upper)\n",
    "\n",
    "print(doc[3].is_alpha)  # check it is alpha or not \n",
    "\n",
    "print(doc[3].is_ascii)  \n",
    "\n",
    "print(doc[3].is_digit)\n",
    "\n",
    "print(doc[-1].is_punct)\n",
    "\n",
    "print(doc[-1].is_left_punct)\n",
    "\n",
    "print(doc[-3].is_left_punct)\n",
    "\n",
    "print(doc[-4].is_right_punct)\n",
    "\n",
    "print(doc[1].is_space)\n",
    "\n",
    "print(doc[-5].is_bracket)\n",
    "\n",
    "print(doc[-6].is_quote)\n",
    "\n",
    "print(doc[6].is_currency)\n",
    "\n",
    "doc = nlp(\"I emailed you at least 100 times\")\n",
    "print(doc[-2].like_num)\n",
    "\n",
    "doc = nlp(\"My email is duygu@packt.com and you can visit me under https://duygua.github.io any time you want.\")\n",
    "print(doc[3].like_email)\n",
    "\n",
    "print(doc[10].like_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4e4ab402-8253-42c6-afaf-cd747ff72f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Girl Xxxx\n",
      "called xxxx\n",
      "Kathy Xxxxx\n",
      "has xxx\n",
      "a x\n",
      "nickname xxxx\n",
      "Cat123 Xxxddd\n",
      ". .\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Girl called Kathy has a nickname Cat123.\")\n",
    "for token in doc:\n",
    "    print(token.text, token.shape_)  \n",
    "    \n",
    "# It returns a string that shows a token's orthographic features.Numbers are replaced with d, uppercase letters are replaced with X, and\n",
    "# -- lowercase letters are replaced with x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "290f64ba-97da-4005-bca1-83df78955b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I True\n",
      "visited True\n",
      "aravind True\n",
      "at True\n",
      "night True\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I visited Jenny at Mynks Resort\")\n",
    "for token in doc1:\n",
    "    print(token, token.is_oov)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4d718866-0c46-4177-9ac8-a1012c7c76da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I True\n",
      "just True\n",
      "want False\n",
      "to True\n",
      "inform False\n",
      "you True\n",
      "that True\n",
      "I True\n",
      "was True\n",
      "with True\n",
      "the True\n",
      "principle False\n",
      ". False\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I just want to inform you that I was with the principle.\")\n",
    "for token in doc:\n",
    "    print(token, token.is_stop)  # it will find this is stopword or not "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f96feb4-324c-4486-a83f-7e382080beed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
